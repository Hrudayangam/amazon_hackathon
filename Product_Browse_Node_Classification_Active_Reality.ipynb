{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Product Browse Node Classification - Active Reality",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hrudayangam/amazon_hackathon/blob/main/Product_Browse_Node_Classification_Active_Reality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6oCtGIdroCQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t8wL82i3czT"
      },
      "source": [
        "#Defining functions for preprocessing of string type data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm97t6kScRYD"
      },
      "source": [
        "def Clean_a_String(string):  \n",
        "\n",
        "  clean_string_list = re.sub('[^a-zA-Z]', ' ', string)\n",
        "  #print(clean_string_list)\n",
        "  clean_string_list = clean_string_list.lower()\n",
        "  #print(clean_string_list)\n",
        "  clean_string_list = clean_string_list.split()\n",
        "  #print(clean_string_list)\n",
        "\n",
        "  ps = PorterStemmer()\n",
        "  all_stopwords = stopwords.words('english')\n",
        "  all_stopwords.remove('not')\n",
        "\n",
        "  clean_string_list = [ps.stem(word) for word in clean_string_list if not word in set(all_stopwords)]\n",
        "  clean_string_list = ' '.join(clean_string_list)\n",
        "  \"\"\"\n",
        "  for i in clean_string_list:\n",
        "    if i in words.words():\n",
        "      continue\n",
        "    else:\n",
        "      clean_string_list.remove(i)\n",
        "  \"\"\" \n",
        "  return clean_string_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVc0esH7eO4X"
      },
      "source": [
        "def Clean_the_data_TITLE(dataset):\n",
        "  List = []\n",
        "  try:  \n",
        "    for index in range(len(dataset)):\n",
        "      string = dataset['TITLE'][index]\n",
        "      if string != string:\n",
        "        temp_list = [\"NOT_REQUIRED\"]\n",
        "\n",
        "      else:\n",
        "        temp_list = str(Clean_a_String(string))\n",
        "        #temp_list += \" \" + str(dataset['BRAND'][index])\n",
        "      \n",
        "      List.append(temp_list)\n",
        "      \n",
        "  except TypeError:\n",
        "    print(dataset['TITLE'][index])\n",
        "  \n",
        "  except KeyError:\n",
        "    print(dataset['TITLE'][index])\n",
        "  \n",
        "  return List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e82Ou6hBj3Hy"
      },
      "source": [
        "def Clean_the_data_OTHER(dataset): \n",
        "  List = []\n",
        "  dataset['DESCRIPTION'] = dataset['DESCRIPTION'].fillna(\"0\")\n",
        "  dataset['BULLET_POINTS'] = dataset['BULLET_POINTS'].fillna(\"0\")\n",
        "\n",
        "  try:  \n",
        "    for index in range(len(dataset)):\n",
        "      string = dataset['DESCRIPTION'][index] + dataset['BULLET_POINTS'][index]\n",
        "      if string != string:\n",
        "        temp_list = []\n",
        "\n",
        "      else:\n",
        "        temp_list = Clean_a_String(string)\n",
        "      \n",
        "      List.append(temp_list)\n",
        "\n",
        "  except TypeError:\n",
        "    print(TypeError)\n",
        "    print(dataset['TITLE'][index])\n",
        "  \n",
        "  except KeyError:\n",
        "    print(KeyError)\n",
        "    print(dataset['TITLE'][index])\n",
        "  \n",
        "  return List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihz78lDi37BE"
      },
      "source": [
        "#Data extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voyw6YlWsiJ6"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Amazon ML/train.csv\", escapechar=\"\\\\\", quoting= csv.QUOTE_NONE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLvHVC2Z4BAU"
      },
      "source": [
        "#Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYbvR7hJVzjl"
      },
      "source": [
        "X = Clean_the_data_TITLE(data)\n",
        "#X = Clean_the_data_OTHER(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un7xcRV0U3EY"
      },
      "source": [
        "*X will take values and x will produce combine 'TITLE' and 'BRAND' data for bag of words*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC7wxjXHopot"
      },
      "source": [
        "x = []\n",
        "for i in range(len(X)):\n",
        "  temp = str(X[i]) + \" \" + str(data['BRAND'][i])\n",
        "  #temp = str(X[i])\n",
        "  x.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt3TuuAtqVR2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer(max_features=1500)\n",
        "x=cv.fit_transform(x).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bnKU9h9UrYE"
      },
      "source": [
        "**Encoding for Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7joK_GjtUMI"
      },
      "source": [
        "y = data.iloc[:, -1].values\n",
        "y = array(y)\n",
        "print(y)\n",
        "# one hot encode\n",
        "y = to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSFlkfg6yte8"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAhaEAYFqu67"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owPd3XaPquJV"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUI-wnPp6dZi"
      },
      "source": [
        "#**ANN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rrc8Sb86cmQ"
      },
      "source": [
        "ann = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbDGbqbL6rWe"
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=1000, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=1000, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=y.shape[1], activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K84HLOzq67sY"
      },
      "source": [
        "ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Qek1Xs7D_Z"
      },
      "source": [
        "ann.fit(x_train, y_train, batch_size = 64, epochs = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MAXHIZGUVBs"
      },
      "source": [
        "#ANN PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2DcjAQ0DsAn"
      },
      "source": [
        "y_pred = ann.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnFczyfGUYA1"
      },
      "source": [
        "#Converting probability into Binary output in each list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d502k1Gaxlfe"
      },
      "source": [
        "y_pred = np.around(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydRUajfx48WN"
      },
      "source": [
        "#**ACCURACY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rx-pQZ8RYqd"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}